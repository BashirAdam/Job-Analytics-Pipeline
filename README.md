# Job Market Analytics Pipeline & Dashboard âœ…

Professional end-to-end portfolio project showing an ETL pipeline that extracts job postings, cleans and stores them, and exposes a deployed interactive dashboard.

---

## ğŸš€ Project Overview

**Goal:** Build a data pipeline that extracts real job postings, transforms and stores them in a database, and visualizes insights via an interactive dashboard.

**Tech stack:** Python (requests, pandas, BeautifulSoup), Streamlit, SQLAlchemy, SQLite/Postgres, Docker, GitHub Actions.


## ğŸ“ Project Structure

```
job-analytics-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractor.py          # Data extraction from Remotive API
â”‚   â”œâ”€â”€ transformer.py        # Data cleaning & transformation
â”‚   â”œâ”€â”€ loader.py             # Load to DB (SQLite/Postgres)
â”‚   â””â”€â”€ etl_pipeline.py       # Main ETL orchestration
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ schema.sql            # Database schema
â”‚   â””â”€â”€ queries.sql           # Analytical queries
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                # Streamlit app
â”‚   â””â”€â”€ components/           # UI helpers
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_etl.py           # Unit tests
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/ci.yml      # CI pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ¯ Quickstart â€” Local (recommended)

1. Create a virtual environment and install dependencies

```bash
python -m venv .venv
. .venv/Scripts/activate  # Windows PowerShell
pip install -r requirements.txt
```

2. Run the ETL (extract -> transform -> load)

```bash
python src/etl_pipeline.py --search "data scientist" --limit 200
```

Outputs:
- `data/raw/remotive_jobs.json`
- `data/processed/jobs_clean.csv`
- `data/jobs.db` (SQLite DB)

3. Start dashboard

```bash
streamlit run dashboard/app.py
# Open http://localhost:8501
```

---

## ğŸ³ Quickstart â€” Docker

Build and run with Docker Compose:

```bash
docker-compose up --build
# App available at http://localhost:8501
```

---

## â˜ï¸ Deploying (Free Hosting options)

- Streamlit Community Cloud: Push repo to GitHub, create a new app in Streamlit Cloud pointing to `dashboard/app.py`. Ensure `requirements.txt` is present.
- Render / Railway: Build Docker image with provided `docker/Dockerfile` and deploy.


## âœ… CI

GitHub Actions workflow runs `pytest` on push/pull requests to `main` branch â€” see `.github/workflows/ci.yml`.

---

## ğŸ§­ Database schema & Analytics

- Initialize DB: `sqlite:///data/jobs.db` is used by default. Schema at `database/schema.sql`.
- Pre-written queries in `database/queries.sql` for quick analysis (top skills, salary stats, etc.).

---

## ğŸ§ª Running tests

```bash
pytest -q
```

---

## ğŸ“¸ Screenshots

> **Add screenshots here** after you run the dashboard. Example placeholders:
>
> - `docs/screenshots/dashboard_kpis.png`
> - `docs/screenshots/top_skills.png`

---

## ğŸ“ Notes

- The extractor uses the public Remotive API (no key) â€” it's ethical to use for this demo.
- For production, add rate-limiting, job deduplication policies, and robust monitoring.

---

## ğŸ§© Next improvements (ideas)

- Normalize skills into a separate table for better analytics
- Add incremental runs and Airflow / Prefect orchestration
- Add authentication and multi-user dashboards

---

## ğŸ“„ License

Use an appropriate license for your portfolio (e.g., MIT).


---

If you want, I can also:
- Add a GitHub Actions step to automatically deploy to Streamlit Cloud
- Create a demo README with screenshots after you run the pipeline

